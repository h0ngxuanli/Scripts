## Why and How transfer learning can learn?
The main idea behind transfer learning is enhancing inter-domain transferability and intra-domain discriminability. $\textit{Ben-Davidâ€™s Theorem}$ demystifies transfer learning and fascinates me greatly. I wrote [survey of UDA.pdf](https://github.com/k0ngyiji/Scripts/blob/master/survey%20of%20UDA.pdf) as a brief summary for my research experience on Unsupervised Domain Adaptation.


## Multi-source domain adaptation
During my research on traditional Unsupervised Domain Adaptation,  I wanna find a cross-atlas diagnose solution for Alzheimer's and [CDLS](https://ieeexplore.ieee.org/document/7780918) inspired me. I derived a multi-site cross-atlas diagnose model from CDLS and reformulated it into a QP form in [multi-source domain adaptation](https://github.com/k0ngyiji/Scripts/blob/master/multi-source%20domain%20adaptation.pdf). This model failed in my data, and actually the multi-site cross-atlas diagnose scenario is not pratical.


## Transformer and Graph Neural Networks
I met a lot of Transformer models during intern and [VectorNet](https://arxiv.org/abs/2005.04259) is the most elegant and inspiring work among them. I wrote [self-attention and GCN.pdf](https://github.com/k0ngyiji/Scripts/blob/master/self-attention%20and%20GCN.pdf) for a group meeting on how to model interactions among traffic agents.
